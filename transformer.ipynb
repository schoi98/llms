{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer\n",
    "\n",
    "The transformer architecture introduced in the paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) has served as the basis for nearly all of the modern day language models.\n",
    "This assumes some understanding of basic deep learning including how CNNs and RNNs work. \n",
    "\n",
    "**Heavily Sourced From**\n",
    "https://nlp.seas.harvard.edu/annotated-transformer/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./assets/transformer.png\" height=\"480\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Decoder\n",
    "\n",
    "The transformer can be broken down into the encoder-decoder architecture (left and right in the figure above respectively)\n",
    "\n",
    "Each step in this architecture is auto-regressive, taking previously generated symbols as additional input when generating text. (???)\n",
    "\n",
    "#### Masking\n",
    "\n",
    "#### Residual Connection\n",
    "https://arxiv.org/abs/1512.03385 \n",
    "\n",
    "#### Layer Normalization\n",
    "https://arxiv.org/abs/1607.06450\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))` where `Sublayer(x)` is the function implemented by sublayer\n",
    "\n",
    "#### Dropout\n",
    "https://jmlr.org/papers/v15/srivastava14a.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Architecture \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The encoder maps an input sequence of symbol/token representation $(x_1, ..., x_n)$ to a sequence of continuous representation $\\mathbf{z} = (z_1, \\dots, z_n)$.\n",
    "\n",
    "\n",
    "#### Encoder Sublayer\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"./assets/transformer-encoder-sublayer.png\" height=\"480\">\n",
    "<figcaption>The `SublayerConnection`</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./assets/transformer-encoder.png\" height=\"480\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Decoder\n",
    "\n",
    "The decoder takes the output of the encoder $\\mathbf{z}$ and outputs a sequence $(y_1, \\dots y_m)$ of symbols/tokens. \n",
    "\n",
    "The output embedding refers to the embedding of the *target* sequence taht is input to the decoder during training.\n",
    "It is the vector representation of the output you want. During training these are usually the correct tokens up to the current position in the inputs, shifted by one to indicate the start of a sequence/start token (<SOS>, etc.).\n",
    "With this we are trying to tell the model to predict the next token in the sequence given the previous tokens.\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./assets/transformer-decoder.png\" height=\"480\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Masking in Decoder\n",
    "\n",
    "We modify the masking in the self-attention sublayer (called look-ahead or causal masking) in the decoder to prevent future tokens in the 'output embedding' from being \"attended\" to.\n",
    "Each token can only attend to itself and tokens before it in the sequence. \n",
    "This + output embeddings are offset by one position, ensures that predictions for position $i$ (current position in the target/output sequence) can depend only on the known outputs at position $< i$, \n",
    "thus enforcing an auto-regressive property where the model can only use information from previous tokens in the sequence to make predictions.\n",
    "\n",
    "This is done by setting the weights for the future tokens to negative infinity before the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    mask = torch.triu(torch.ones(attn_shape, dtype=torch.bool), diagonal=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Attention Mechanism\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/scaled-dot-attention.png\" height=\"360\">\n",
    "</p>\n",
    "\n",
    "An attention function can be thought of as a mapping of queries and a set of key-value pairs to an output. \n",
    "The output is a weighted sum of the values where the weight is calculated by a 'compatibility function' between the query and key vectors.\n",
    "The attention used in the original paper is called the \"Scaled Dot-Product Attention\".\n",
    "The inputs consist of queries $Q$ and keys $K$ of dimension $d_k$ and values $V$ of dimension $d_v$. \n",
    "We compute the dot product between the query and all the keys, scale by $\\sqrt{d_k}$, add an optional mask, apply the softmax function to obtain the weights for the values.\n",
    "In practive we compute the attention function on a set of queries simultaneously, in a matrix $Q$ $d_k \\times d_k$.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "#### Alternate Attention Mechanism\n",
    "\n",
    "- Additive attention: Uses single layer feed-forward layer (slower and less space efficient even though theoretically same complexity).\n",
    "- Dot product attention: Same as in the paper except for scaling factor of $\\frac{1}{\\sqrt{d_k}}$\n",
    "\n",
    "The scaling factor is added as additive attention outperforms dot product attention for larger values of $d_k$ ([https://arxiv.org/abs/1703.03906](Paper)), \n",
    "suspectd that for large $d_k$ the dot product grows large in magnitude pushing softmax fucntion into regions of small gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead Attention\n",
    "Multi-head attention combines multiple attention mechanisms in parallel to allow the model to jointly attend to different mappings learned by each component. \n",
    "\n",
    "$$\\begin{align}\n",
    "  \\text{MultiHead}(Q, K, V) &= \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O\\\\\n",
    "  \\text{where head}_\\text{i} &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\\end{align}$$\n",
    "\n",
    "Where each. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/multihead-attention.png\" height=\"360\">\n",
    "</p>\n",
    "\n",
    "**TODO:** needs better explanation\n",
    "\n",
    "The multi-head attention mechanism is used in three ways in the transformer\n",
    "\n",
    "  1. Encoder-Decoder Attention: The queries come from previous decoder layers, the memory keys and values come from the output of the encoder. This means that all positions in the decoder attends over all positions in the input sequence (similar to Seq2Seq).\n",
    "  2. Encoder Attention: Has self-attention layesr where all of the keys, values, and queries come from the sample place. In an encoder this is the output of trhe previous layer. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
    "  3. Decoder Attention: Similar to encoder attention, allows each position in the decoder to attend to all positions in the decoder up to and including that decoder (preserve auto-regressive property)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self, h, d_model, dropout=0.1):\n",
    "    super().__init__()\n",
    "    assert d_model % h == 0\n",
    "    self.d_k = d_model // h # assume d_v = d_k\n",
    "    self.h = h\n",
    "    self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "    self.attn = None\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "  def forward(self, query, key, value, mask=None):\n",
    "    if mask is not None:\n",
    "      mask = mask.unsqueeze(1)\n",
    "    nbatches = query.size(0)\n",
    "    # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "    query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                         for l, x in zip(self.linears, (query, key, value))]\n",
    "    # 2) Apply attention on all the projected vectors in batch.\n",
    "    x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "    # 3) \"Concat\" using a view and apply a final linear.\n",
    "    x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "    del query, key, value\n",
    "    return self.linears[-1](x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network\n",
    "\n",
    "Each layer of the encoder and decoder also contains a fully connected feed-forward network. \n",
    "\n",
    "**TODO:** why tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "We use learned embeddings to convert the input and output tokens to vectors of dimension $d_{model}$.\n",
    "\n",
    "We use the learned linear transformation and softmax fucntion to convert the decoder output to next-token probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "The current model does not have a way to encode the position and order of the tokens yet. To do this we add positional encodings to the bottom of the encoder and decoder stacks. These have dimesion $d_{model}$ so that they can be summed.\n",
    "\n",
    "The original paper makes use of the following positional encoding function\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin(pos/10000^{2i/d_{model}})\\\\\n",
    "PE_{(pos, 2i + 1)} = \\cos(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension, each dimension of the positional encoding corresponds to a sinusoid. \n",
    "\n",
    "**TODO:** why these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, dropout, max_len=5000):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0)\n",
    "    self.register_buffer('pe', pe)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = x + self.pe[:, :x.size(1)]\n",
    "    return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = EncoderDecoder(\n",
    "    encoder=Encoder(EncoderLayer(512, MultiheadAttention(8, 512), FFN(512, 2048), 0.1), 6),\n",
    "    decoder=Decoder(DecoderLayer(512, MultiheadAttention(8, 512), MultiheadAttention(8, 512), FFN(512, 2048), 0.1), 6),\n",
    "    src_embed=nn.Sequential(Embeddings(512, 1000), PositionalEncoding(512, 0.1)),\n",
    "    tgt_embed=nn.Sequential(Embeddings(512, 1000), PositionalEncoding(512, 0.1)),\n",
    "    generator=Generator(512, 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Uses in the Real World\n",
    "\n",
    "### BERT\n",
    "\n",
    "\n",
    "### GPT \n",
    "\n",
    "\n",
    "### LLAMA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
